# -*- coding: utf-8 -*-
"""Untitled46.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DP7XuHD7dZSKcyqGdHPs0pEpvcYTpAjN
"""

# Step 0: install & import libraries
!pip install seaborn scikit-learn --quiet

import numpy as np
import pandas as pd

import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.ensemble import RandomForestRegressor

plt.rcParams['figure.figsize'] = (10, 5)
plt.rcParams['axes.grid'] = True

!file economy.csv

import pandas as pd


try:
    df = pd.read_csv("economy.csv", encoding="utf-16")
    print("UTF-16 OK")
    display(df.head())
except:
    print("UTF-16 failed")

try:
    df = pd.read_excel("economy.csv")
    print("Excel OK")
    display(df.head())
except:
    print("Excel failed")
try:
    df = pd.read_csv("economy.csv", sep=None, engine="python")
    print("Autodetect OK")
    display(df.head())
except:
    print("Autodetect failed")

# Step 1: basic overview
df.head()

df.info()

df.describe(include='all').T

df = pd.read_csv("economy.csv", encoding="utf-8", engine="python", on_bad_lines="skip")
df.head()

# Step 2: clean price column

df['price'] = (
    df['price']
        .astype(str)
        .str.replace(r'[^0-9]', '', regex=True)
)

# пустые строки превращаем в NaN, потом в 0
df['price'] = df['price'].replace('', '0').astype(int)

df['price'].head()

# Step 3: process date column

df['date'] = pd.to_datetime(df['date'], format='%d-%m-%Y', errors='coerce')

df['journey_day'] = df['date'].dt.day
df['journey_month'] = df['date'].dt.month
df['journey_year'] = df['date'].dt.year
df['journey_weekday'] = df['date'].dt.day_name()

df[['date', 'journey_day', 'journey_month', 'journey_year', 'journey_weekday']].head()

# Step 4: process departure and arrival time

df['dep_time'] = pd.to_datetime(df['dep_time'], format='%H:%M', errors='coerce')
df['arr_time'] = pd.to_datetime(df['arr_time'], format='%H:%M', errors='coerce')

df['dep_hour'] = df['dep_time'].dt.hour
df['dep_minute'] = df['dep_time'].dt.minute

df['arr_hour'] = df['arr_time'].dt.hour
df['arr_minute'] = df['arr_time'].dt.minute

df[['dep_time', 'arr_time', 'dep_hour', 'arr_hour']].head()

# Step 5: convert time_taken ("2h 10m") to minutes (robust version)

def time_to_minutes(x):
    x = str(x).strip().lower()
    h = 0
    m = 0
    parts = x.split()

    for p in parts:
        if 'h' in p:
            val = p.replace('h', '')
            if val.isdigit():
                h = int(val)
        elif 'm' in p:
            val = p.replace('m', '')
            if val.isdigit():
                m = int(val)

    return h * 60 + m

df['time_minutes'] = df['time_taken'].apply(time_to_minutes)

df[['time_taken', 'time_minutes']].head()

# Step 6: convert stop info into numeric stops

import re

def extract_stops(s):
    s = str(s).lower().strip()

    # non-stop or direct
    if 'non' in s:
        return 0

    # find any digit in the string
    match = re.search(r'\d+', s)
    if match:
        return int(match.group())

    # no info → assume 0
    return 0

df['num_stops'] = df['stop'].apply(extract_stops)

df[['stop', 'num_stops']].head()

# Step 7: rename key columns to avoid conflicts
df = df.rename(columns={
    'from': 'source',
    'to': 'destination'
})

df.columns

# Step 8: drop unused columns

columns_to_drop = [
    'date',
    'dep_time',
    'arr_time',
    'time_taken',
    'stop'
]

df_model = df.drop(columns=columns_to_drop, errors='ignore')

df_model.head()

# Step 9: One-Hot Encode categorical columns

from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder

categorical_cols = ['airline', 'source', 'destination', 'journey_weekday']
numeric_cols = [col for col in df_model.columns if col not in categorical_cols + ['price']]

preprocessor = ColumnTransformer(
    transformers=[
        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)
    ],
    remainder='passthrough'
)

# Step 10: Train/Test split

from sklearn.model_selection import train_test_split

X = df_model.drop(columns=['price'])
y = df_model['price']

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

len(X_train), len(X_test)

X_train.dtypes

# Remove leftover datetime columns if they exist
for col in df_model.columns:
    if "time" in col and df_model[col].dtype == 'datetime64[ns]':
        df_model[col] = df_model[col].astype(str)

from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestRegressor

categorical_cols = ['airline', 'source', 'destination', 'journey_weekday']
numeric_cols = [col for col in df_model.columns if col not in categorical_cols + ['price']]

preprocessor = ColumnTransformer(
    transformers=[
        ('categorical', OneHotEncoder(handle_unknown='ignore'), categorical_cols)
    ],
    remainder='passthrough'   # leave numeric columns as is
)

model = Pipeline(steps=[
    ('preprocess', preprocessor),
    ('rf', RandomForestRegressor(
        n_estimators=200,
        random_state=42,
        n_jobs=-1
    ))
])

# Step 11 (fixed): define categorical & numeric columns correctly
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestRegressor

# 1) все object-колонки считаем категориальными
categorical_cols = df_model.select_dtypes(include=['object']).columns.tolist()
print("Categorical columns:", categorical_cols)

# 2) все остальные, кроме целевой price, считаем числовыми
numeric_cols = [c for c in df_model.columns if c not in categorical_cols + ['price']]
print("Numeric columns:", numeric_cols)

# 3) ColumnTransformer: OHE для категориальных, остальное пропускаем как есть
preprocessor = ColumnTransformer(
    transformers=[
        ('categorical', OneHotEncoder(handle_unknown='ignore'), categorical_cols)
    ],
    remainder='passthrough'
)

# 4) Pipeline: Preprocess + RandomForest
model = Pipeline(steps=[
    ('preprocess', preprocessor),
    ('rf', RandomForestRegressor(
        n_estimators=300,
        random_state=42,
        n_jobs=-1
    ))
])

# Step 12A: check and drop NaN rows
df_model.isnull().sum()

# Step 12B: recreate X, y, and train/test split after dropping NaNs

X = df_model.drop(columns=['price'])
y = df_model['price']

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

len(X_train), len(X_test)

# Step 12: rebuild X, y and handle NaNs safely

# 1) Берём только те данные, которые уже есть в df_model
X = df_model.drop(columns=['price']).copy()
y = df_model['price'].copy()

# 2) Находим категориальные и числовые колонки заново
cat_cols = X.select_dtypes(include=['object']).columns.tolist()
num_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()

print("Categorical:", cat_cols)
print("Numeric:", num_cols)

# 3) Заполняем пропуски:
#    - категориальные → 'Unknown'
#    - числовые → медиана по колонке
X[cat_cols] = X[cat_cols].fillna('Unknown')
for c in num_cols:
    X[c] = X[c].fillna(X[c].median())

# Проверка, что NaN больше нет
print("Any NaN in X:", X.isnull().any().any())

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

X_train.shape, X_test.shape

model = Pipeline(steps=[
    ('preprocess', preprocessor),
    ('rf', RandomForestRegressor(
        n_estimators=50,   # вместо 100
        max_depth=12,
        random_state=42,
        n_jobs=-1
    ))
])

model.fit(X_train, y_train)

y_pred = model.predict(X_test)
y_pred[:10]

from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np

mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
r2 = r2_score(y_test, y_pred)

mae, rmse, r2

import matplotlib.pyplot as plt

plt.figure(figsize=(6,6))
plt.scatter(y_test, y_pred, s=2, alpha=0.5)
plt.xlabel("Real Values")
plt.ylabel("Predicted Values")
plt.title("Actual vs Predicted")
plt.plot([y_test.min(), y_test.max()],
         [y_test.min(), y_test.max()],
         color='red')
plt.show()

from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np

mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
r2 = r2_score(y_test, y_pred)

mae, rmse, r2

import joblib
joblib.dump(model, "flight_price_model.pkl")

import pandas as pd

feat_importances = model.named_steps['rf'].feature_importances_
columns = model.named_steps['preprocess'].get_feature_names_out()

imp_df = pd.DataFrame({
    'feature': columns,
    'importance': feat_importances
}).sort_values(by='importance', ascending=False)

imp_df.head(20)

df[['airline', 'source', 'destination', 'price']].sort_values('price').head(20)

df.groupby('num_stops')['price'].mean()

df.groupby('journey_month')['price'].mean().sort_values()

df.groupby('airline')['price'].mean().sort_values().head(10)

df.groupby(['source', 'destination'])['price'].mean().sort_values().head(10)

import matplotlib.pyplot as plt
import pandas as pd

# 1) средняя цена по авиакомпаниям
airline_avg_price = (
    df.groupby('airline')['price']
      .mean()
      .sort_values()
      .head(10)              # 10 самых дешёвых
      .reset_index()
)

airline_avg_price

plt.figure(figsize=(10, 6))

plt.barh(
    airline_avg_price['airline'],
    airline_avg_price['price']
)

plt.xlabel("Average Ticket Price")
plt.ylabel("Airline")
plt.title("Top 10 Cheapest Airlines (Average Price)")

# чтобы самый дешёвый был сверху
plt.gca().invert_yaxis()

# подписи на барах (цены справа)
for i, v in enumerate(airline_avg_price['price']):
    plt.text(v + 50, i, f"{int(v):,}", va='center')

plt.tight_layout()
plt.show()

